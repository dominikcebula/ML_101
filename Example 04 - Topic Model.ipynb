{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_ROOT = 'Example04'\n",
    "NEWSGROUP_JSON_PATH = f'{EXAMPLE_ROOT}/newsgroups.json'\n",
    "NEWSGROUP_CONTENT_RAW_TXT_PATH = f'{EXAMPLE_ROOT}/01_newsgroups_content_raw.txt'\n",
    "NEWSGROUP_LINES_RAW_TXT_PATH = f'{EXAMPLE_ROOT}/02_newsgroups_lines_raw.txt'\n",
    "NEWSGROUP_CLEAN_TXT_PATH = f'{EXAMPLE_ROOT}/03_newsgroups_clean.txt'\n",
    "NEWSGROUP_LEMINIZED_PATH = f'{EXAMPLE_ROOT}/04_leminized.txt'\n",
    "NEWSGROUP_LEMINIZED_NO_STOP_PATH = f'{EXAMPLE_ROOT}/05_leminized_no_stop.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>From: tchen@magnus.acs.ohio-state.edu (Tsung-K...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>From: a207706@moe.dseg.ti.com (Robert Loper)\\n...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>From: kimman@magnus.acs.ohio-state.edu (Kim Ri...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>From: kwilson@casbah.acns.nwu.edu (Kirtley Wil...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>Subject: Re: Don't more innocents die without ...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>From: livesey@solntze.wpd.sgi.com (Jon Livesey...</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>From: dls@aeg.dsto.gov.au (David Silver)\\nSubj...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>Subject: Re: Mike Francesa's 1993 Predictions\\...</td>\n",
       "      <td>9</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>From: jet@netcom.Netcom.COM (J. Eric Townsend)...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10008</th>\n",
       "      <td>From: gld@cunixb.cc.columbia.edu (Gary L Dare)...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>From: sehari@iastate.edu (Babak Sehari)\\nSubje...</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  target  \\\n",
       "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "10     From: irwin@cmptrc.lonestar.org (Irwin Arnstei...       8   \n",
       "100    From: tchen@magnus.acs.ohio-state.edu (Tsung-K...       6   \n",
       "1000   From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...       2   \n",
       "10000  From: a207706@moe.dseg.ti.com (Robert Loper)\\n...       7   \n",
       "10001  From: kimman@magnus.acs.ohio-state.edu (Kim Ri...       6   \n",
       "10002  From: kwilson@casbah.acns.nwu.edu (Kirtley Wil...       2   \n",
       "10003  Subject: Re: Don't more innocents die without ...       0   \n",
       "10004  From: livesey@solntze.wpd.sgi.com (Jon Livesey...       0   \n",
       "10005  From: dls@aeg.dsto.gov.au (David Silver)\\nSubj...       1   \n",
       "10006  Subject: Re: Mike Francesa's 1993 Predictions\\...       9   \n",
       "10007  From: jet@netcom.Netcom.COM (J. Eric Townsend)...       8   \n",
       "10008  From: gld@cunixb.cc.columbia.edu (Gary L Dare)...      10   \n",
       "10009  From: sehari@iastate.edu (Babak Sehari)\\nSubje...      12   \n",
       "\n",
       "                  target_names  \n",
       "0                    rec.autos  \n",
       "1        comp.sys.mac.hardware  \n",
       "10             rec.motorcycles  \n",
       "100               misc.forsale  \n",
       "1000   comp.os.ms-windows.misc  \n",
       "10000                rec.autos  \n",
       "10001             misc.forsale  \n",
       "10002  comp.os.ms-windows.misc  \n",
       "10003              alt.atheism  \n",
       "10004              alt.atheism  \n",
       "10005            comp.graphics  \n",
       "10006       rec.sport.baseball  \n",
       "10007          rec.motorcycles  \n",
       "10008         rec.sport.hockey  \n",
       "10009          sci.electronics  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(NEWSGROUP_JSON_PATH)\n",
    "\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def sentence_to_words(sentences):\n",
    "    return [sent.split(' ') for sent in sentences]\n",
    "        \n",
    "def dump_to_file(filePath, data):\n",
    "    with open(filePath, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(\"%s\\n\" % item.encode(\"utf-8\"))\n",
    "\n",
    "def leminize_data_words(data_words):\n",
    "    return [lemmatizer.lemmatize(word) for word in data_words]\n",
    "\n",
    "def filter_out_stop_words(data_words):\n",
    "    return [word for word in data_words if word not in stopWords]\n",
    "\n",
    "def remove_empty_words(data_words):\n",
    "    return [word for word in data_words if word]\n",
    "\n",
    "def join(data_words_sentences):\n",
    "    return [' '.join(sent) for sent in data_words_sentences]\n",
    "\n",
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "\n",
    "dump_to_file(NEWSGROUP_CONTENT_RAW_TXT_PATH, data)\n",
    "\n",
    "# Remove unrequired entries\n",
    "data = [re.sub('Re:', '', sent, flags = re.I) for sent in data]\n",
    "data = [re.sub('From:.*', '', sent, flags = re.I) for sent in data]\n",
    "data = [re.sub('Organization:.*', '', sent, flags = re.I) for sent in data]\n",
    "data = [re.sub('Nntp-Posting-Host:.*', '', sent, flags = re.I) for sent in data]\n",
    "data = [re.sub('Distribution:.*', '', sent, flags = re.I) for sent in data]\n",
    "data = [re.sub('Article-I.D.:.*', '', sent, flags = re.I) for sent in data]\n",
    "data = [re.sub('Keywords:.*', '', sent, flags = re.I) for sent in data]\n",
    "data = [re.sub('Expires:.*', '', sent, flags = re.I) for sent in data]\n",
    "data = [re.sub('Subject:', '', sent, flags = re.I) for sent in data]\n",
    "data = [re.sub('Lines: [0-9]+', '', sent, flags = re.I) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "dump_to_file(NEWSGROUP_LINES_RAW_TXT_PATH, data)\n",
    "\n",
    "# Replace unwanted characters\n",
    "data = [re.sub('[^A-Za-z]', ' ', sent) for sent in data]\n",
    "\n",
    "# transform to lowercase\n",
    "data = [sent.lower() for sent in data]\n",
    "\n",
    "# remove double spaces\n",
    "data = [re.sub(\"\\ +\", \" \", sent) for sent in data]\n",
    "\n",
    "# trim sentences\n",
    "data = [sent.strip() for sent in data]\n",
    "\n",
    "dump_to_file(NEWSGROUP_CLEAN_TXT_PATH, data)\n",
    "\n",
    "# convert sentences to words\n",
    "data_words_sentences = sentence_to_words(data)\n",
    "\n",
    "# perform leminization\n",
    "data_words_sentences = [leminize_data_words(data_words) for data_words in data_words_sentences]\n",
    "\n",
    "dump_to_file(NEWSGROUP_LEMINIZED_PATH, join(data_words_sentences))\n",
    "\n",
    "# filter out stop words\n",
    "data_words_sentences = [filter_out_stop_words(data_words) for data_words in data_words_sentences]\n",
    "\n",
    "# remove empty words\n",
    "data_words_sentences = [remove_empty_words(data_words) for data_words in data_words_sentences]\n",
    "\n",
    "dump_to_file(NEWSGROUP_LEMINIZED_NO_STOP_PATH, join(data_words_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
